%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Plantilla: para la realizaci�n de informes.
%Curso:     Simulaci�n estad�stica.
%Profesor:  Johann A. Ospina.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Establece el tipo de documento (art�culo), tama�o de letra (10pt) a una columna.
\documentclass[letterpaper,12pt,onecolumn,titlepage]{article} 
 
 
% Cargar paquetes
\usepackage{verbatim}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{ucs}
\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage{anysize}
\usepackage{fancyhdr}
\usepackage[comma,authoryear]{natbib}
\usepackage{url} %paquete para definir url
\usepackage{hyperref}  %hipervinculos

%Estilo de la p�gina
\pagestyle{fancy}

%Establecer el margen
\marginsize{2cm}{2cm}{1cm}{1cm}
\setlength{\headheight}{13.1pt}


% Portada
\title{
    \textbf{Laboratorio N.2}\
    ~\\{Introduccion a Los Metodos Estadisticos}   
    ~\\{Generacion de Estimadores}}
\author{
    {Diana Carolina Arias Sinisterra Cod. 1528008}
 ~\\{Kevin Steven Garcia Chica Cod. 1533173}
 ~\\{Cesar Andres Saavedra Vanegas Cod. 1628466}}

\date{
     \textbf{Universidad Del Valle}\   
    ~\\{Facultad De Ingenieria}
    ~\\{Estadistica}
    ~\\{Octubre}
    ~\\{2017}}
 
 
 
\decimalpoint %Poner punto decimal
 
\begin{document}
 
% Se aplica el formato a las p�ginas. Se despliegan: portada e �ndices de materias, figuras y tablas
\renewcommand{\listtablename}{}
\renewcommand{\tablename}{Tabla}
\maketitle
\setcounter{page}{2}
\tableofcontents{}
%\thispagestyle{empty}
%\newpage
%\listoffigures{}
%\listoftables

\thispagestyle{empty}

\newpage
\fancyhead{}
\fancyfoot{}
 
% Encabezado y pie de pagina
\lhead{Introduccion a los Metodos Estadisticos}
\lfoot{Universidad Del Valle}
\rfoot{\thepage}

% Estilo de la bibliograf�a
\bibliographystyle{apalike}
 
% Desarrollo de los contenidos del documento
\pagebreak\section{Situaci\'{o}n 1}
\subsection{Punto a.}
~\\ Un estimador m\'{a}ximo veros\'{i}mil de $\lambda$ para una funci\'{o}n Poisson$(\lambda)$ esta dado por.

~\\ $$f_{(x)}(x) = \frac{\exp^{-\lambda}\lambda^{X}}{X!}$$

~\\ $L(x,\lambda) = \prod \limits_{i=1}^n{}\frac{\exp^{-\lambda}\lambda^{X}}{X!}$

~\\ $L(x,\lambda) = \frac{\exp^{-\lambda n}\lambda^{\sum\limits_{i=1}^n{X}}}{ \prod \limits_{i=1}^n [x_{i}!]}$
~\\ $Ln(L(x,\lambda)) = Ln\left(\frac{\exp^{-\lambda n}\lambda^{\sum\limits_{i=1}^n{X}}}{ \prod \limits_{i=1}^n [x_{i}!]}\right)$

~\\ $Ln(L(x,\lambda)) = -\lambda n + Ln(\lambda^{\sum\limits_{i=1}^n{x_{i}}}) - Ln(\prod \limits_{i=1}^n [x_{i}!])$
~\\ $Ln(L(x,\lambda)) = -\lambda n + {\sum\limits_{i=1}^n{x_{i}}}Ln(\lambda) - Ln(\prod \limits_{i=1}^n [x_{i}!])$

~\\ $\frac{\partial(Ln(L(x;\lambda)))}{\partial\lambda} = \frac{\partial}{\partial\lambda}(-\lambda n + {\sum\limits_{i=1}^n{x_{i}}}Ln(\lambda) - Ln(\prod \limits_{i=1}^n [x_{i}!]))$

~\\ $\frac{\partial(Ln(L(x;\lambda)))}{\partial\lambda} = \frac{\sum\limits_{i=1}^n{x_{i}}}{\lambda} - n $
~\\ $\frac{\sum\limits_{i=1}^n{x_{i}}}{\lambda} - n = 0$

~\\ $\frac{\sum\limits_{i=1}^n{x_{i}}}{\lambda} = n $
~\\ $\hat{\lambda} = \frac{\sum\limits_{i=1}^n{x_{i}}}{n}$

~\\ \textbf{Donde $\hat{\lambda}$ es un estimador m\'{a}ximo veros\'{i}mil e insesgado para la funci\'{o}n de distribuci\'{o}n poisson.} 
 
~\\ $\therefore \hat{\lambda} = \bar{x}$

\subsection{Punto b.}
~\\ En un estimador insesgado puesto que la esperanza es igual al par\'{a}metro;


~\\ $E[\hat{\lambda}] = E[\frac{\sum\limits_{i=1}^n{x_{i}}}{n}]$
~\\ $E[\hat{\lambda}] = \frac{1}{n}E[\sum\limits_{i=1}^n{x_{i}}]$
~\\ $E[\hat{\lambda}] = \frac{1}{n}(\sum\limits_{i=1}^{n} E[x])$
~\\ $E[\hat{\lambda}] = E[x]$
~\\ $\hat{\lambda} = \bar{x}$

~\\ Donde $\hat{\lambda}$ es un estimador insesgado para la funci\'{o}n poisson de par\'{a}metro $(\lambda)$. 

~\\ La varianza esta dada por: 


~\\ $Var[\hat{\lambda}]= var[\frac{\sum\limits_{i=1}^n{x_{i}}}{n}]$
~\\ $Var[\hat{\lambda}]= \frac{1}{n^2} var[\sum\limits_{i=1}^n{x_{i}}]$
~\\ $Var[\hat{\lambda}]= \frac{1}{n} var[x_{i}]$

~\\ $Var[\hat{\lambda}]= \frac{\lambda}{n}$

\subsection{Punto c.}
~\\ Para calcular la probabilidad de que en un d\'{i}a particular se reciban m\'{a}ximo 2 quejas, es decir $P[x<2|\hat{y}=3]$ a partir de la muestra que que cuenta con una media de $\hat{y}=3$ se usa la funci\'{o}n de densidad de la distribuci\'{o}n de poisson con par\'{a}metro $\lambda=3$. 

~\\ $P[x\le2]= \frac{\exp^{-\lambda}\lambda^{X}}{X!}$

~\\ $P[x\le2]= \frac{\exp^{-3}3^{0}}{0!} + 
			 \frac{\exp^{-3}3^{1}}{1!} + 
			 \frac{\exp^{-3}3^{2}}{2!}$
			 
~\\ $P[x\le2]= 0.4231 $

~\\ Por lo cual la probabilidad que la tiene oficina de recibir como m\'{a}ximo dos quejas en un d\'{i}a es del {$42.31\%$}

\pagebreak\section{Situaci\'{o}n 2}
$$f(y,\lambda,\gamma)=\lambda e^{-\lambda(y-\gamma)}$$
\subsection{Punto a.}
~\\Estimaci\'{o}n de $\lambda$ y $\gamma$ por m\'{a}xima verosimilitud:
~\\ Empezamos con la estimaci\'{o}n de $\lambda$:
~\\ $L(\lambda,\gamma |y1,..yn)=\prod \limits_{i=1}^{n}(\lambda e^{-\lambda (y-\gamma)})=\lambda^n e^{-\lambda \sum \limits_{i=1}^{n}({y_{i}-\gamma})}=\lambda^n e^{-\lambda (\sum \limits_{i=1}^{n}{y_{i}}-n\gamma)}=\lambda^n e^{-\lambda\sum\limits_{i=1}^{n}{y_{i}}+n\lambda\gamma}$
~\\ $Ln(L(\lambda,\gamma |y1,..yn)=nLn(\lambda)+n\lambda\gamma-\lambda\sum\limits_{i=1}^{n}{y_{i}}$
~\\ $\frac{\partial(Ln(L(\lambda,\gamma |y1,..yn)))}{\partial\lambda}=\frac{n}{\lambda}+n\gamma-\sum\limits_{i=1}^{n}{y_{i}}$
~\\Entonces:
$$\frac{n}{\lambda}+n\gamma-\sum\limits_{i=1}^{n}{y_{i}}=0$$
~\ $$\frac{n}{\lambda}=\sum\limits_{i=1}^{n}{y_{i}}-n\gamma$$
~\ $$\frac{1}{\lambda}=\bar{y}-\gamma$$
~\ En conclusi\'{o}n:
$$\hat{\lambda}=\frac{1}{\bar{y}-\gamma}$$
~\\ Ahora, la estimaci\'{o}n para $\gamma$ sera:

~\\ $\frac{\partial(Ln(L(\lambda,\gamma |y1,..yn)))}{\partial\gamma}= n$
~\\ Podemos ver que en la derivada parcial se nos desaparece el par\'{a}metro de inter\'{e}s $\gamma$, sabemos que lo que se quiere con este m\'{e}todo es maximizar la funci\'{o}n de verosimilitud. Entonces, observando nuestra funci\'{o}n de verosimilitud, tenemos:
~\\ $L(\lambda,\gamma |y1,..yn)=\lambda^n e^{-\lambda\sum\limits_{i=1}^{n}{y_{i}}+n\lambda\gamma}$
~\\ Tomando todas las variables en la anterior expresi\'{o}n como constantes excepto $\gamma$ , para maximizar dicha funci\'{o}n, $\gamma$  debe ser lo mas peque\~{n}o posible, ya que con ello, el exponente $-\lambda\sum\limits_{i=1}^{n}{y_{i}}+n\lambda\gamma$ es mas peque\~{n}o y por tanto la exponencial va a ser mayor, haciendo m\'{a}xima toda la expresi\'{o}n.
~\\ En conclusi\'{o}n:
$$\hat{\gamma}=y_(1)= Min\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace$$
 
\subsection{Punto b.} 
~\\ Si observamos detalladamente la funci\'{o}n de densidad, vemos que es una funci\'{o}n de una distribuci\'{o}n exponencial con $x=y-\gamma$
~\\ Haciendo la sustituci\'{o}n anterior tenemos:
$$f(y,\lambda,\gamma)=\lambda e^{-\lambda(y-\gamma)}=\lambda e^{-\lambda x}$$
~\\ Como denotamos $x=y-\gamma$, despejando $y$, nos queda $y=x+\gamma$
~\\ \textbf{Estimaci\'{o}n para el promedio:}
~\\ $E[y]=E[x+\gamma]=E[x]+E[\gamma]$ ; como x es exponencial $E[x]=\frac{1}{\lambda}$ y como $\gamma$ es constante, $E[\gamma]=\gamma$
~\\ Entonces:
~\\ $E[y]=\frac{1}{\lambda}+\gamma$
~\\ Por la propiedad de la invarianza, reemplazando nuestro estimador para $\lambda$:
$$\hat{E[y]}=\frac{1}{\frac{1}{\bar{y}-\gamma}}+\gamma=\bar{y}-\gamma+\gamma=\bar{y}$$
~\ En conclusi\'{o}n:
$$\hat{E[y]}=\bar{y}$$
~\\ \textbf{Estimaci\'{o}n para la varianza:}
~\\ $V[y]=V[x+\gamma]=V[x]+V[\gamma]$ ; como x es exponencial $V[x]=\frac{1}{\lambda^2}$ y como $\gamma$ es constante, $V[\gamma]=0$
~\\ Entonces:
~\\ $V[y]=\frac{1}{\lambda^2}$
~\\ Teniendo en cuenta la propiedad de la invarianza, reemplazando nuestro estimador para $\lambda$ tenemos:
~\\ $\hat{V[y]}=\frac{1}{(\frac{1}{\bar{y}-\gamma})^2}$, observamos que depende de $\gamma$, como el estimador para $\gamma$ es m\'{a}ximo veros\'{i}mil, tambi\'{e}n se puede aplicar la propiedad de la invarianza, entonces, reemplazando tenemos:
~\\ $$\hat{V[y]}=\frac{1}{(\frac{1}{\bar{y}-y_{(1)}})^2}=(y-y_{(1)})^2$$
~\ En conclusi\'{o}n:
$$\hat{V[y]}=(y-y_{(1)})^2$$
~\\ \textbf{Estimaci\'{o}n para la mediana:}
~\\ $Me[y]=Me[x+\gamma]=Me[x]+Me[\gamma]$ ; como $\gamma$ es constante, $Me[\gamma]=\gamma$. Por otra parte, la mediana de x debemos hallarla.
~\\ Sabemos que la mediana esta definida como el punto que nos acumula una probabilidad de 0.5, entonces para hallarla se hace el siguiente procedimiento: 
~\\ $Me[x]=\int \limits_{?}^{M}\lambda e^{-\lambda x}dx=0.5$  Podemos ver que el limite superior lo denotamos como M, el cual es la la mediana. y el limite inferior debemos hallarlo.
~\\ como $x=y-\gamma$ y $0<\gamma<y<\infty$ por lo tanto $0<x<\infty$
~\\ La integral quedar\'{i}a:
~\\ $$Me[x]=\int \limits_{0}^{M}\lambda e^{-\lambda x}dx=0.5$$
~\\ $$=\lambda \int \limits_{0}^{M} e^{-\lambda x}dx=0.5$$ 
~\\ Sea $u=-\lambda x$, $du=-\lambda dx$
~\\ $$=-\frac{\lambda}{\lambda}\int \limits_{0}^{M} e^{u} du=0.5$$
$$=-[e^{-\lambda x}-1]=0.5$$
$$=1-e^{-\lambda x}=0.5$$
$$=e^{-\lambda M}=0.5$$
$$=-\lambda M=Ln(0.5)$$
$$M=-\frac{Ln(0.5)}{\lambda}$$
~\\ Entonces:
$$Me[x]=-\frac{Ln(0.5)}{\lambda}$$
~\\ Por tanto:
$$Me[y]=Me[x]+\gamma=-\frac{Ln(0.5)}{\lambda}+\gamma$$
~\\ Ahora, aplicando la propiedad de la invarianza. Reemplazando nuestros estimadores, nos queda:
~\\ $$Me[y]=-\frac{Ln(0.5)}{\frac{1}{\bar{y}-y_{(1)}}}+y_{(1)}=y_{(1)}-(\bar{y}-y_{(1)})Ln(0.5)$$
~\\En conclusi\'{o}n:
$$\hat{Me[y]}=y_{(1)}-(\bar{y}-y_{(1)})Ln(0.5)$$
\subsection{Punto c.} 
~\\ Como el fabricante piensa igualar la garant\'{i}a a la de su competencia que es de 2 a\~{n}os (730 d\'{i}as), es decir, si el componente se da\~{n}a en menos de 2 a\~{n}os, este tendr\'{a} que reponerlo. Queremos saber que porcentaje de componentes deber\'{i}a reponer al igualar su garant\'{i}a con la de la competencia.
~\\ En s\'{i}mbolos matem\'{a}ticos, tendr\'{i}amos que hallar $P[y<730]$ ya que la variable y esta dada en d\'{i}as.
~\\ la muestra obtenida sobre 10 componentes fue: 730, 780, 740, 650, 670, 800, 1000, 1110, 900, 450. Por tanto $\bar{y}=783$
~\\ Como la funci\'{o}n de distribuci\'{o}n de y depende de $\lambda$ y $\gamma$, tendremos que utilizar nuestros estimadores para calcular estos par\'{a}metros.
~\\ Hallaremos primero $\gamma$:
~\\ $$\hat{\gamma}=y_{(1)}=450$$
~\\ Ahora, hallaremos $\lambda$:
 $$\hat{\lambda}=\frac{1}{\bar{y}-\gamma}=\frac{1}{783-450}=0.003$$
~\\ Entonces: 
~\\ Como f(y) es exponencial con $x=y-\gamma$, entonces $P[y<c]=1-e^{-\lambda(y-\gamma)}$
~\\ En nuestro caso:
~\\ $P[y<730|\bar{y}=783]=1-e^{-0.003(730-450)}=1-e^{-0.8408}=0.5686$


~\\ \textbf{El fabricante tendr\'{a} que reponer aproximadamente el $56.86\%$ de los componentes.}
\pagebreak\section{Situaci\'{o}n 4}
\subsection{Punto a.}
~\\ $$f(y;\theta)=e^{-(y-\theta)};  y>\theta$$  
~\\ \textbf{ESTIMACI\'{O}N POR MOMENTOS: $M'_1=\mu'_1$}
~\\ $M'_1=\frac{1}{n}\sum_{i=1}^{n}{y_{i}}=\bar{y}$
~\\ $\mu'_1=E[Y]=\int \limits_{\theta}^{\infty} y f(y) \cdot dy$
~\\ $=\int \limits_{\theta}^{\infty} ye^{-(y-\theta)} \cdot dy=\int \limits_{\theta}^{\infty} ye^{-y}e^\theta \cdot dy=e^\theta\int \limits_{\theta}^{\infty}ye^{-y} \cdot dy$
~\\ Aplicando integraci\'{o}n por partes: $u=y$ ,$dv=e^{-y}$, $du=dy$ y $v=-e^{-y}$ Nos queda:
~\\ $E[Y]=e^{\theta}[-ye^{-y}+\int \limits_{\theta}^{\infty}e^{-y} \cdot dy] =e^{\theta}[-ye^{-y}- e^{-y}|_{\theta}^{\infty}] $
~\\ $=e^{\theta}(\theta e^{-\theta}+e^{-\theta})=\theta e^{\theta}e^{-\theta}+e^{\theta}e^{-\theta}= \theta + 1 $ 
~\\ Entonces, por el m\'{e}todo de los momentos obtenemos el siguiente estimador:
~\\ $$\mu'_1=\theta + 1= \bar{y} = M'_1$$
 $$\hat{\theta}=\bar{y}-1$$
~\\ \textbf{ESTIMACI\'{O}N POR EL M\'{E}TODO DE M\'{A}XIMA VEROSIMILITUD:}

~\\ $L(\theta; y_{1},y_{2},...,y_{n})=\prod \limits_{i=0}^{n}(e^{-(y-\theta)})=e^{-\sum \limits_{i=1}^n {(y_{i}-\theta)}}=e^{-\sum \limits_{i=1}^n y_{i} + n\theta}$
~\\ Aplicando logaritmo natural:
~\\ $Ln(L(\theta; y_{1},y_{2},...,y_{n}))=-\sum \limits_{i=1}^n y_{i} + n\theta$
~\\ Derivando parcialmente respecto a $\theta$ , nos queda:

~\\ $\frac{\partial(Ln(L(\theta;y_{1},y_{2},...,y_{n})))}{\partial\theta}= n$
~\\ Podemos ver que al derivar parcialmente con respecto a $\theta$, se nos desaparece nuestro par\'{a}metro de inter\'{e}s. Por lo cual debemos analizar nuestra funcion de verosimilitud. 
~\\ Entonces nuestra funcion de verosimilitud es: 
$$L(\theta; y_{1},y_{2},...,y_{n})=e^{-\sum \limits_{i=1}^n y_{i} + n\theta}$$
~\\ Sabemos que este metodo busca maximizar dicha funcion, tomando como constante a $n$ y la expresion $\sum \limits_{i=1}^n y_{i}$, y dejando variable nuestro parametro $\theta$, podemos ver que la funcion se maximiza cuando $\theta$ toma el valor mas grande que pueda tomar en su dominio, ya que con ello $n\theta$ sera mas grande y en general, la expresion $-\sum \limits_{i=1}^n y_{i} + n\theta$ sera mayor, haciendo esto que toda la funcion de verosimilitud sea maxima. 
~\\ Ahora, como el dominio de la funcion de densidad nos dice que $y>\theta$, $\theta$ sera maxima cuando sea igual al $Max\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace$
~\\ \textbf{En conclusion:}
$$\hat{\theta}=Max\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace=y_{(n)}$$

\subsection{Punto b.}
~\\ Ahora veremos si los estimadores hallados en el punto anterior son insesgados:
~\\ Estimador por el metodo de los momentos:
~\\ $\hat{\theta_{MM}}=\bar{y}-1$
~\\ $E[\hat{\theta_{MM}}]=E[\bar{y}-1]=E[\bar{y}]-E[1]=E[\frac{1}{n}\sum \limits_{i=1}^{n} y_{i}]-1=\frac{1}{n} E[\sum \limits_{i=1}^{n} y_{i}]-1=\frac{1}{n}(n(1+\theta))-1=1+\theta-1=\theta$
~\\ \textbf{El estimador $\hat{\theta_{MM}}=\bar{y}-1$ es insesgado, ya que  $E[\hat{\theta_{MM}}]=\theta$}

~\\ Estimador por el metodo de maxima verosimilitud:
~\\ $\hat{\theta_{MV}}=Max\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace$
~\\ $E[\hat{\theta_{MV}}]=E[Max\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace]=E[y_{(n)}]=\theta+1$
~\\ Entonces nuestro estimador  $\hat{\theta_{MV}}=Max\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace$ no es insesgado, ya que $E[\hat{\theta_{MV}}]\neq \theta$
~\\ Entonces, aplicando una transformacion a nuestro estimador para que este sea insesgado, tenemos:

~\\ $$\hat{\theta^*_{MV}}=Max\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace - 1 = y_{n}-1$$
~\\ $$E[\hat{\theta^*_{MV}}]=E[y_{(n)}-1]=E[y_{(n)}]-E[1]=\theta+1-1=\theta$$
~\\ \textbf{En conclusion, el estimador $\hat{\theta^*_{MV}}=Max\left\lbrace{y_{1},y_{2},y_{3},...,y_{n}}\right\rbrace - 1 = y_{n}-1$ es insesgado para el parametro $\theta$, ya que, $\hat{\theta^*_{MV}}=\theta$ } 
\pagebreak\section{Situaci\'{o}n 5}
\subsection{Punto a.}
~\\ $$f(x;\theta) = \frac{2\theta^2}{x^3} ; \theta<x<\infty $$
~\\ $M1'=\sum_{i=1}^{n}\frac{x_{i}}{n}=\bar{x}$
~\\ $\mu'_1 =?$
~\\ $\mu'_1=E[X]= \int \limits_{\theta}^{\infty} x f(x) \cdot dx$
~\\ $E[X]=\int \limits_{\theta}^{\infty} x \frac{2\theta^2}{x^3}\cdot dx=\int \limits_{\theta}^{\infty}\frac{2\theta^2}{x^2}\cdot dx$
~\\ $E[X]=2\theta^2 \int \limits_{\theta}^{\infty}\frac{1}{x^2}\cdot dx= 2\theta^2[-\frac{1}{x}|_{\theta}^{\infty}]=2\theta^2(\frac{1}{\theta})=2\theta$
~\\ $$\mu'_1=E[X]=2\theta=\bar{X}=M1'$$
~\ $$\hat{\theta}=\frac{\bar{X}}{2}$$
~\\ \textbf{En conclusi\'{o}n, el estimador por el m\'{e}todo de los momentos para $\theta$ de la funci\'{o}n de densidad $f(x;\theta) = \frac{2\theta^2}{x^3} ; \theta<x<\infty $ es $\hat{\theta}=\frac{\bar{X}}{2}$}


\pagebreak\section{Situaci\'{o}n 7}
~\ Sean $Y_{1}, Y_{2}, Y_{3},...,Y_{n}$ una muestra aleatoria extra\'{i}da de una poblaci\'{o}n con funci\'{o}n de densidad:

~\ $$f({y})= \frac{1}{2\theta +2}  ;  -1<Y<2\theta+1$$
~\ Donde; $f({y}) ~ Uniforme(a=-1,b=2\theta+1)$

\subsection{Punto a.}
~\ Un estimador m\'{a}ximo veros\'{i}mil para $\theta$ y $\sigma^{2}$ son:

~\\ \textbf{Para $\theta$ :}

~\\ $L(y;\theta) = \prod\limits_{i=1}^n{}(\frac{1}{2\theta +2})$
~\\ $L(y;\theta) = (\frac{1}{2\theta +2})^{n}$
~\\ $Ln(L(y;\theta)) = Ln((\frac{1}{2\theta +2})^{n})$
~\\ $Ln(L(y;\theta)) = n[Ln(\frac{1}{2\theta +2})]$
~\\ $Ln(L(y;\theta)) = n[Ln(1)-Ln({2\theta+2})]$
~\\ $Ln(L(y;\theta)) = n[-Ln({2\theta+2})]$
~\\ $\frac{\partial (Ln(L(y;\theta)))}{\partial\theta} = \frac{\partial}{\partial\theta}(n[-Ln({2\theta+2})])$

~\\ $\hat{\theta} = \frac{n}{\theta+1}$

~\\ Donde el par\'{a}metro es el limite superior de la variaci\'{o}n de la funci\'{o}n de distribuci\'{o}n. 

~\\ $\therefore \hat{\theta} = Maximo \left\lbrace{y_{1}, y_{2}, y_{3},...,y_{n}}\right\rbrace$ 

~\\ \textbf{Para $\sigma^{2}$ :}
~\\ Como sabemos que $f(y)$ es uniforme con $a=-1$ y $b=2\theta+1$, tenemos que la varianza es:
~\\ $\sigma^{2} = Var(Y) = \frac{(b-a)^2}{12}$
~\\ $Var(Y) = \frac{(2\theta+1-(-1))^2}{12}$
~\\ $Var(Y) = \frac{(2\theta+2)^2}{12}$
~\\ $Var(Y) = \frac{4\theta^2 +8\theta+4}{12}=\frac{4(\theta^2 +2\theta+1)}{12}=\frac{\theta^2 +2\theta+1}{3}$
~\\ $Var(Y) = \frac{(\theta+1)^2}{3}$
~\\ Por la propiedad de la invarianza de los estimadores m\'{a}ximo veros\'{i}miles, tenemos que una estimaci\'{o}n para $\sigma^2$ sera:
~\\ $\hat{\sigma^2}=\frac{(Y_{(n)}+1)^2}{3}$

\subsection{Punto b.}
~\ La estimaci\'{o}n por momentos para $\theta$ sera:
~\\ $M'_1=\bar{Y}$
~\\ $\mu'_1=E[Y]=?$
~\\ $E[Y]= \frac{(a+b)}{2}$  , ya que f(y) tiene distribuci\'{o}n uniforme
~\\ $E[Y]= \frac{(-1+(2\theta+1))}{2}$
~\\ $E[Y]= \frac{(2\theta)}{2}$
~\\ $E[Y]= \theta$
~\\ Entonces: $M'_1=\bar{Y}=\theta=\mu'_1$
~\\ Por tanto: $\hat{\theta}=\bar{Y}$

\bibliography{Bibliografia}
\end{document}